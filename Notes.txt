There is no universal crawling script.
1. Play the rules : read the website policy and robots.txt file
2. Analyze the returning contents. The following added the add_header. 
3. Use browser (also try different browsers to find things) to inspect Network requests and responses. Every website is different. Find the
useful url (xhr response, search related if that's the purpose) in the meaningful files (eg, search.json)..
4. Url contents download, dynamic webpage scraping

req = urllib.request.Request('https://www.xxx.ca/find/abc.html?cid=search-abc')
req.add_header('Accept', 'application/json,application/xml')
urllib.request.urlopen(req).read().decode('utf-8').find('Market ') > 0
returningstring=urllib.request.urlopen(req).read().decode('utf-8')
with open("page.html", "w") as file:
    file.write(returningstring)

htmlstring=download('https://www.sportchek.ca/services/sportchek/search-and-promote/products?q=pants+men+helly&page=1&count=18',)
with open("sportschek.html", "w") as file:
    file.write(htmlstring)

It will return the search results. Can change display page and count in the url so you get all the results.
